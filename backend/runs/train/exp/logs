[INFO] 2022-08-21 17:27:40,914 run: Running torch.distributed.run with args: ['/root/miniconda3/lib/python3.8/site-packages/torch/distributed/run.py', '--nproc_per_node', '2', 'train.py', '--batch', '128', '--data', 'coco.yaml', '--weights', 'yolov5s.pt', '--device', '0,1']
[INFO] 2022-08-21 17:27:40,915 run: Using nproc_per_node=2.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[INFO] 2022-08-21 17:27:40,915 api: Starting elastic_operator with launch configs:
  entrypoint       : train.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:29500
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

[INFO] 2022-08-21 17:27:40,916 local_elastic_agent: log directory set to: /tmp/torchelastic_df2nfyhk/none_8giw5luo
[INFO] 2022-08-21 17:27:40,916 api: [default] starting workers for entrypoint: python
[INFO] 2022-08-21 17:27:40,916 api: [default] Rendezvous'ing worker group
[INFO] 2022-08-21 17:27:40,916 static_tcp_rendezvous: Creating TCPStore as the c10d::Store implementation
/root/miniconda3/lib/python3.8/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
[INFO] 2022-08-21 17:27:40,917 api: [default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=29500
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

[INFO] 2022-08-21 17:27:40,917 api: [default] Starting worker group
[INFO] 2022-08-21 17:27:40,918 __init__: Setting worker0 reply file to: /tmp/torchelastic_df2nfyhk/none_8giw5luo/attempt_0/0/error.json
[INFO] 2022-08-21 17:27:40,918 __init__: Setting worker1 reply file to: /tmp/torchelastic_df2nfyhk/none_8giw5luo/attempt_0/1/error.json
train: weights=yolov5s.pt, cfg=, data=coco.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=300, batch_size=128, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=0,1, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest
Command 'git fetch origin' timed out after 5 seconds
YOLOv5 🚀 v6.1-267-g50ff6ee Python-3.8.10 torch-1.9.0+cu111 CUDA:0 (NVIDIA GeForce RTX 3090, 24268MiB)
                                                            CUDA:1 (NVIDIA GeForce RTX 3090, 24268MiB)

Added key: store_based_barrier_key:1 to store for rank: 0
Rank 0: Completed store-based barrier for 2 nodes.
hyperparameters: lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0
Weights & Biases: run 'pip install wandb' to automatically track and visualize YOLOv5 🚀 runs in Weights & Biases
ClearML: run 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML
TensorBoard: Start with 'tensorboard --logdir runs/train', view at http://localhost:6006/

                 from  n    params  module                                  arguments                     
  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              
  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                
  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   
  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               
  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 
  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              
  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 
  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              
  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 
  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 
 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              
 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          
 12           [-1, 6]  1         0  models.common.Concat                    [1]                           
 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          
 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              
 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          
 16           [-1, 4]  1         0  models.common.Concat                    [1]                           
 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          
 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              
 19          [-1, 14]  1         0  models.common.Concat                    [1]                           
 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          
 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              
 22          [-1, 10]  1         0  models.common.Concat                    [1]                           
 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          
 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]
Model summary: 270 layers, 7235389 parameters, 7235389 gradients, 16.6 GFLOPs

Transferred 349/349 items from yolov5s.pt
AMP: checks passed ✅
optimizer: SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.001), 60 bias
train: Scanning '/root/autodl-tmp/datasets/coco/train2017.cache' images and labels... 117266 found, 1021 missing, 0 empty, 0 corrupt: 100%|██████████| 118287/118287 [00:00<?, ?it/s]                              
val: Scanning '/root/autodl-tmp/datasets/coco/val2017.cache' images and labels... 4952 found, 48 missing, 0 empty, 0 corrupt: 100%|██████████| 5000/5000 [00:00<?, ?it/s]                                          
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)

AutoAnchor: 4.45 anchors/target, 0.995 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅
Plotting labels to runs/train/exp/labels.jpg... 
Image sizes 640 train, 640 val
Using 16 dataloader workers
Logging results to runs/train/exp
Starting training for 300 epochs...

     Epoch   gpu_mem       box       obj       cls    labels  img_size
     0/299     14.1G    0.0424     0.058   0.01691       731       640:   0%|          | 1/925 [00:05<1:25:40,  5.56s/it]                                                                                          Reducer buckets have been rebuilt in this iteration.
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
     0/299     14.2G   0.04376   0.06334   0.01617       741       640:   4%|▍         | 38/925 [00:16<04:28,  3.30it/s]                                                                                           train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
     0/299     14.2G   0.04387   0.06301   0.01611       935       640:  33%|███▎      | 303/925 [01:33<03:02,  3.40it/s]                                                                                          fatal: unable to access 'https://github.com/ultralytics/yolov5/': gnutls_handshake() failed: The TLS connection was non-properly terminated.
     0/299     14.2G    0.0441   0.06308   0.01624       645       640:  61%|██████▏   | 568/925 [02:50<01:43,  3.45it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
     0/299     14.1G    0.0444   0.06328   0.01649        90       640: 100%|██████████| 925/925 [04:35<00:00,  3.36it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:40<00:00,  1.02s/it]                                                                            
                 all       5000      36335      0.633      0.489      0.522      0.325

     Epoch   gpu_mem       box       obj       cls    labels  img_size
     1/299     14.2G   0.04656   0.06475   0.01918       113       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.77it/s]                                                                            
                 all       5000      36335      0.595      0.434      0.467      0.278

     Epoch   gpu_mem       box       obj       cls    labels  img_size
     2/299     14.2G    0.0508   0.07017    0.0256        91       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.74it/s]                                                                            
                 all       5000      36335      0.514      0.344      0.355      0.194

     Epoch   gpu_mem       box       obj       cls    labels  img_size
     3/299     14.2G   0.05435     0.073   0.03077       115       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.76it/s]                                                                            
                 all       5000      36335      0.519      0.353      0.367      0.206

     Epoch   gpu_mem       box       obj       cls    labels  img_size
     4/299     14.2G   0.05394   0.07264   0.03011        84       640: 100%|██████████| 925/925 [04:29<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:24<00:00,  1.65it/s]                                                                            
                 all       5000      36335      0.521       0.38      0.392      0.222

     Epoch   gpu_mem       box       obj       cls    labels  img_size
     5/299     14.2G   0.05354   0.07241   0.02962       107       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:24<00:00,  1.61it/s]                                                                            
                 all       5000      36335      0.564      0.389      0.414      0.242

     Epoch   gpu_mem       box       obj       cls    labels  img_size
     6/299     14.2G   0.05341   0.07216   0.02955       820       640:  11%|█         | 101/925 [00:29<04:00,  3.43it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
     6/299     14.2G   0.05332   0.07227   0.02931        92       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.74it/s]                                                                            
                 all       5000      36335      0.574      0.408      0.437      0.258

     Epoch   gpu_mem       box       obj       cls    labels  img_size
     7/299     14.2G   0.05297   0.07198    0.0289       147       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.70it/s]                                                                            
                 all       5000      36335      0.584      0.428      0.455      0.274

     Epoch   gpu_mem       box       obj       cls    labels  img_size
     8/299     14.2G   0.05291   0.07203   0.02885       139       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:24<00:00,  1.64it/s]                                                                            
                 all       5000      36335      0.585      0.431      0.461      0.279

     Epoch   gpu_mem       box       obj       cls    labels  img_size
     9/299     14.2G   0.05279   0.07171   0.02856       139       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:24<00:00,  1.67it/s]                                                                            
                 all       5000      36335      0.601      0.437      0.474       0.29

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    10/299     14.2G   0.05265   0.07214   0.02863       921       640:  44%|████▎     | 403/925 [01:56<02:29,  3.49it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    10/299     14.2G   0.05267   0.07186   0.02854        96       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.67it/s]                                                                            
                 all       5000      36335      0.606      0.444       0.48      0.295

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    11/299     14.2G   0.05262   0.07156   0.02827       131       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:24<00:00,  1.66it/s]                                                                            
                 all       5000      36335      0.606      0.455       0.49      0.304

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    12/299     14.2G   0.05249   0.07118    0.0282       946       640:  60%|██████    | 556/925 [02:41<01:46,  3.46it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    12/299     14.2G   0.05249   0.07125   0.02819       108       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.72it/s]                                                                            
                 all       5000      36335      0.613      0.456      0.493      0.306

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    13/299     14.2G   0.05244    0.0716   0.02825        73       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.73it/s]                                                                            
                 all       5000      36335      0.606      0.465      0.498      0.311

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    14/299     14.2G   0.05242   0.07147   0.02807       769       640:  78%|███████▊  | 718/925 [03:28<00:59,  3.51it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    14/299     14.2G   0.05239   0.07132   0.02808       150       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.72it/s]                                                                            
                 all       5000      36335      0.617      0.468      0.502      0.314

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    15/299     14.2G   0.05231   0.07143   0.02815       965       640:  61%|██████    | 566/925 [02:44<01:45,  3.42it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    15/299     14.2G   0.05231   0.07159    0.0281       119       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.69it/s]                                                                            
                 all       5000      36335       0.61      0.471      0.504      0.316

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    16/299     14.2G   0.05243   0.07203   0.02806       115       640: 100%|██████████| 925/925 [04:26<00:00,  3.47it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.71it/s]                                                                            
                 all       5000      36335      0.611      0.474      0.506      0.317

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    17/299     14.2G   0.05235   0.07135   0.02803       147       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.70it/s]                                                                            
                 all       5000      36335      0.621      0.472      0.506      0.318

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    18/299     14.2G   0.05231   0.07173   0.02796       112       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.72it/s]                                                                            
                 all       5000      36335      0.621      0.471      0.507      0.319

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    19/299     14.2G   0.05224   0.07139   0.02788       912       640:  95%|█████████▍| 876/925 [04:13<00:14,  3.45it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    19/299     14.2G   0.05224   0.07137   0.02787        77       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.71it/s]                                                                            
                 all       5000      36335      0.618      0.474      0.509       0.32

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    20/299     14.2G   0.05223   0.07161   0.02798       100       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.72it/s]                                                                            
                 all       5000      36335      0.617      0.473      0.509       0.32

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    21/299     14.2G   0.05218    0.0714   0.02794        98       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.71it/s]                                                                            
                 all       5000      36335      0.623       0.47       0.51      0.321

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    22/299     14.2G   0.05239    0.0718   0.02815       937       640:  14%|█▎        | 126/925 [00:36<03:49,  3.48it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    22/299     14.2G   0.05223   0.07137   0.02787        85       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.71it/s]                                                                            
                 all       5000      36335      0.622      0.471       0.51      0.321

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    23/299     14.2G   0.05213   0.07138   0.02783       836       640:  71%|███████   | 656/925 [03:09<01:18,  3.43it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    23/299     14.2G   0.05215   0.07152   0.02782        81       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.72it/s]                                                                            
                 all       5000      36335      0.623      0.471       0.51      0.322

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    24/299     14.2G   0.05216    0.0716    0.0277       122       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.72it/s]                                                                            
                 all       5000      36335      0.621      0.473      0.511      0.322

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    25/299     14.2G   0.05209   0.07151   0.02775        76       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.70it/s]                                                                            
                 all       5000      36335       0.62      0.474      0.511      0.322

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    26/299     14.2G   0.05214   0.07166   0.02788        71       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.72it/s]                                                                            
                 all       5000      36335      0.619      0.475       0.51      0.322

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    27/299     14.2G   0.05215   0.07161   0.02772       100       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.72it/s]                                                                            
                 all       5000      36335       0.62      0.474      0.511      0.322

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    28/299     14.2G    0.0522   0.07114   0.02786       869       640:   5%|▌         | 50/925 [00:14<04:14,  3.44it/s]                                                                                           train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    28/299     14.2G   0.05212    0.0713   0.02769       865       640:  28%|██▊       | 258/925 [01:14<03:11,  3.49it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    28/299     14.2G   0.05215   0.07174   0.02779       161       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.70it/s]                                                                            
                 all       5000      36335      0.625      0.472      0.511      0.322

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    29/299     14.2G   0.05209   0.07146   0.02767       106       640: 100%|██████████| 925/925 [04:26<00:00,  3.47it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.72it/s]                                                                            
                 all       5000      36335      0.625      0.472      0.511      0.322

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    30/299     14.2G   0.05201   0.07164   0.02759       109       640: 100%|██████████| 925/925 [04:26<00:00,  3.47it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.72it/s]                                                                            
                 all       5000      36335      0.624      0.471      0.511      0.322

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    31/299     14.2G   0.05208   0.07145   0.02764       159       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.73it/s]                                                                            
                 all       5000      36335      0.626       0.47      0.512      0.322

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    32/299     14.2G   0.05209   0.07145    0.0275       820       640:  63%|██████▎   | 587/925 [02:49<01:39,  3.41it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    32/299     14.2G   0.05209   0.07145    0.0275       119       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.71it/s]                                                                            
                 all       5000      36335      0.629       0.47      0.512      0.322

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    33/299     14.2G   0.05205   0.07162   0.02762       113       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.72it/s]                                                                            
                 all       5000      36335      0.629       0.47      0.512      0.322

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    34/299     14.2G   0.05199   0.07137   0.02753       699       640:  80%|████████  | 742/925 [03:35<00:53,  3.44it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    34/299     14.2G   0.05201   0.07138   0.02759        60       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.72it/s]                                                                            
                 all       5000      36335      0.628      0.472      0.512      0.322

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    35/299     14.2G   0.05201   0.07131   0.02758        57       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.72it/s]                                                                            
                 all       5000      36335      0.632      0.469      0.512      0.322

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    36/299     14.2G   0.05191   0.07168   0.02744       930       640:  97%|█████████▋| 900/925 [04:20<00:07,  3.42it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    36/299     14.2G    0.0519   0.07163   0.02743        65       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.73it/s]                                                                            
                 all       5000      36335      0.632      0.469      0.512      0.322

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    37/299     14.2G   0.05194   0.07157   0.02746       134       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.73it/s]                                                                            
                 all       5000      36335      0.629      0.472      0.512      0.322

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    38/299     14.2G   0.05192   0.07155   0.02739       115       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.73it/s]                                                                            
                 all       5000      36335      0.626      0.472      0.512      0.322

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    39/299     14.2G   0.05171   0.07104   0.02725       813       640:  14%|█▍        | 133/925 [00:38<03:56,  3.35it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    39/299     14.2G    0.0518   0.07143   0.02741       702       640:  62%|██████▏   | 577/925 [02:47<01:40,  3.47it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    39/299     14.2G    0.0519   0.07144   0.02745        73       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.73it/s]                                                                            
                 all       5000      36335      0.627      0.473      0.513      0.323

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    40/299     14.2G   0.05193   0.07161   0.02738       141       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.74it/s]                                                                            
                 all       5000      36335      0.625      0.473      0.513      0.323

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    41/299     14.2G    0.0519    0.0712   0.02747        61       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.73it/s]                                                                            
                 all       5000      36335      0.624      0.471      0.513      0.324

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    42/299     14.2G   0.05191   0.07165   0.02736       108       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.74it/s]                                                                            
                 all       5000      36335      0.625      0.471      0.513      0.324

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    43/299     14.2G   0.05169   0.07112   0.02739       863       640:  50%|████▉     | 458/925 [02:12<02:18,  3.36it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    43/299     14.2G   0.05179   0.07108   0.02738       100       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.74it/s]                                                                            
                 all       5000      36335      0.632      0.469      0.513      0.324

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    44/299     14.2G    0.0518   0.07151   0.02727       157       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.75it/s]                                                                            
                 all       5000      36335      0.629      0.471      0.513      0.324

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    45/299     14.2G   0.05175   0.07129   0.02723       136       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.71it/s]                                                                            
                 all       5000      36335       0.63       0.47      0.513      0.324

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    46/299     14.2G   0.05167   0.07238   0.02727       784       640:   4%|▎         | 34/925 [00:09<04:16,  3.47it/s]                                                                                           train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    46/299     14.2G    0.0517   0.07137   0.02723       105       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.75it/s]                                                                            
                 all       5000      36335      0.632      0.471      0.514      0.324

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    47/299     14.2G   0.05173   0.07109   0.02731       108       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.73it/s]                                                                            
                 all       5000      36335      0.626      0.474      0.514      0.324

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    48/299     14.2G   0.05166   0.07103   0.02723        56       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.74it/s]                                                                            
                 all       5000      36335      0.627      0.473      0.514      0.324

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    49/299     14.2G   0.05188   0.07211   0.02733       938       640:  15%|█▌        | 139/925 [00:40<03:46,  3.47it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    49/299     14.2G   0.05173   0.07137   0.02723        65       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.73it/s]                                                                            
                 all       5000      36335      0.629      0.472      0.515      0.325

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    50/299     14.2G   0.05174   0.07122   0.02716       119       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.74it/s]                                                                            
                 all       5000      36335      0.634       0.47      0.515      0.325

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    51/299     14.2G   0.05171   0.07103   0.02718        63       640: 100%|██████████| 925/925 [04:26<00:00,  3.47it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.72it/s]                                                                            
                 all       5000      36335      0.629      0.471      0.515      0.325

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    52/299     14.2G   0.05168    0.0711   0.02727        90       640: 100%|██████████| 925/925 [04:30<00:00,  3.43it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.75it/s]                                                                            
                 all       5000      36335      0.626      0.474      0.515      0.325

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    53/299     14.2G   0.05173   0.07162   0.02696       875       640:  48%|████▊     | 447/925 [02:10<02:22,  3.36it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    53/299     14.2G   0.05168   0.07144     0.027       166       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.73it/s]                                                                            
                 all       5000      36335      0.625      0.474      0.515      0.325

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    54/299     14.2G   0.05165    0.0714   0.02704       133       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.72it/s]                                                                            
                 all       5000      36335      0.626      0.474      0.515      0.326

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    55/299     14.2G   0.05167   0.07132   0.02708       852       640:  34%|███▍      | 315/925 [01:31<03:00,  3.38it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    55/299     14.2G   0.05159   0.07135   0.02701        93       640: 100%|██████████| 925/925 [04:29<00:00,  3.43it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.73it/s]                                                                            
                 all       5000      36335      0.625      0.475      0.515      0.326

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    56/299     14.2G   0.05168    0.0715   0.02709        91       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.74it/s]                                                                            
                 all       5000      36335      0.626      0.474      0.516      0.326

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    57/299     14.2G   0.05164   0.07119   0.02692       147       640: 100%|██████████| 925/925 [04:29<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.75it/s]                                                                            
                 all       5000      36335      0.628      0.474      0.516      0.326

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    58/299     14.2G   0.05155   0.07104   0.02703        74       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.74it/s]                                                                            
                 all       5000      36335      0.627      0.475      0.516      0.326

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    59/299     14.2G   0.05166   0.07116   0.02697       735       640:  67%|██████▋   | 620/925 [02:58<01:27,  3.49it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    59/299     14.2G   0.05162   0.07103   0.02693       797       640:  86%|████████▌ | 797/925 [03:50<00:37,  3.44it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    59/299     14.2G   0.05159   0.07088   0.02696        93       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.70it/s]                                                                            
                 all       5000      36335      0.627      0.475      0.516      0.326

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    60/299     14.2G   0.05155   0.07111   0.02694       106       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.73it/s]                                                                            
                 all       5000      36335      0.626      0.477      0.516      0.327

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    61/299     14.2G   0.05144   0.07109   0.02698       112       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.75it/s]                                                                            
                 all       5000      36335      0.624      0.478      0.517      0.327

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    62/299     14.2G   0.05156   0.07118   0.02683       101       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.73it/s]                                                                            
                 all       5000      36335      0.628      0.477      0.517      0.327

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    63/299     14.2G   0.05139    0.0707   0.02678        62       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.76it/s]                                                                            
                 all       5000      36335      0.629      0.477      0.517      0.327

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    64/299     14.2G   0.05149    0.0714   0.02679       777       640:  19%|█▊        | 173/925 [00:50<03:36,  3.47it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    64/299     14.2G   0.05142   0.07106   0.02678        85       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.74it/s]                                                                            
                 all       5000      36335       0.63      0.477      0.518      0.327

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    65/299     14.2G   0.05146   0.07103   0.02678        62       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.74it/s]                                                                            
                 all       5000      36335      0.631      0.475      0.518      0.327

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    66/299     14.2G    0.0515   0.07114   0.02683       743       640:  36%|███▌      | 329/925 [01:35<02:57,  3.36it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    66/299     14.2G   0.05145   0.07103   0.02682        60       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.77it/s]                                                                            
                 all       5000      36335       0.63      0.476      0.518      0.328

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    67/299     14.2G    0.0514   0.07102    0.0267       112       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.74it/s]                                                                            
                 all       5000      36335      0.626      0.479      0.518      0.328

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    68/299     14.2G   0.05128   0.07047    0.0267       791       640:  53%|█████▎    | 488/925 [02:20<02:04,  3.50it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    68/299     14.2G   0.05133   0.07068   0.02671       123       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.76it/s]                                                                            
                 all       5000      36335       0.63      0.478      0.518      0.328

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    69/299     14.2G   0.05137   0.07097   0.02673       973       640:  82%|████████▏ | 756/925 [03:38<00:49,  3.42it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    69/299     14.2G   0.05137   0.07096   0.02677        91       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.75it/s]                                                                            
                 all       5000      36335      0.631      0.478      0.518      0.327

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    70/299     14.2G   0.05143   0.07092    0.0267       144       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.76it/s]                                                                            
                 all       5000      36335      0.633      0.476      0.518      0.328

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    71/299     14.2G    0.0513   0.07086    0.0267        92       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.76it/s]                                                                            
                 all       5000      36335      0.633      0.478      0.519      0.328

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    72/299     14.2G   0.05131   0.07125   0.02668        76       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.74it/s]                                                                            
                 all       5000      36335      0.633      0.477       0.52      0.328

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    73/299     14.2G   0.05122   0.07062   0.02654        97       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.75it/s]                                                                            
                 all       5000      36335       0.63       0.48       0.52      0.328

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    74/299     14.2G   0.05125   0.07206   0.02663       800       640:  14%|█▍        | 134/925 [00:38<03:43,  3.53it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    74/299     14.2G   0.05118   0.07135   0.02649       815       640:  33%|███▎      | 306/925 [01:28<02:56,  3.51it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    74/299     14.2G   0.05122   0.07085   0.02657       116       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.75it/s]                                                                            
                 all       5000      36335      0.631       0.48       0.52      0.329

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    75/299     14.2G   0.05124   0.07098   0.02645       115       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.76it/s]                                                                            
                 all       5000      36335      0.634      0.478       0.52      0.329

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    76/299     14.2G   0.05125   0.07073   0.02656       148       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.74it/s]                                                                            
                 all       5000      36335      0.635      0.477       0.52      0.329

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    77/299     14.2G   0.05116   0.07048    0.0266       768       640:  15%|█▌        | 142/925 [00:40<03:45,  3.48it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    77/299     14.2G   0.05125   0.07078   0.02661        70       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.74it/s]                                                                            
                 all       5000      36335      0.634      0.476      0.521      0.329

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    78/299     14.2G   0.05114   0.07069   0.02653       105       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.76it/s]                                                                            
                 all       5000      36335       0.64      0.475      0.521       0.33

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    79/299     14.2G   0.05116   0.07066   0.02643        57       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.74it/s]                                                                            
                 all       5000      36335      0.636      0.478      0.521       0.33

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    80/299     14.2G   0.05106   0.07078   0.02639        98       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.75it/s]                                                                            
                 all       5000      36335      0.639      0.477      0.521       0.33

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    81/299     14.2G   0.05116   0.07125   0.02642       853       640:  48%|████▊     | 448/925 [02:09<02:19,  3.41it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    81/299     14.2G   0.05111   0.07105   0.02644       800       640:  77%|███████▋  | 713/925 [03:26<01:00,  3.48it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    81/299     14.2G   0.05113   0.07102   0.02642        82       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.76it/s]                                                                            
                 all       5000      36335       0.64      0.478      0.521      0.329

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    82/299     14.2G   0.05118   0.07104   0.02631       105       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.76it/s]                                                                            
                 all       5000      36335      0.638      0.478      0.521      0.329

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    83/299     14.2G   0.05107   0.07068   0.02641       107       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.75it/s]                                                                            
                 all       5000      36335      0.636      0.479      0.522      0.329

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    84/299     14.2G   0.05112   0.07098   0.02629       764       640:  56%|█████▌    | 514/925 [02:28<01:57,  3.51it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    84/299     14.2G   0.05112   0.07104   0.02634        69       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.76it/s]                                                                            
                 all       5000      36335      0.635      0.479      0.522       0.33

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    85/299     14.2G   0.05103    0.0708   0.02627       172       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.76it/s]                                                                            
                 all       5000      36335      0.633      0.479      0.522       0.33

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    86/299     14.2G   0.05101   0.07059   0.02627       101       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.75it/s]                                                                            
                 all       5000      36335      0.632      0.479      0.523      0.331

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    87/299     14.2G   0.05104    0.0704    0.0262       865       640:  90%|█████████ | 837/925 [04:02<00:25,  3.43it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    87/299     14.2G   0.05104   0.07047   0.02621        65       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.75it/s]                                                                            
                 all       5000      36335      0.633      0.479      0.523      0.331

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    88/299     14.2G   0.05092   0.07058   0.02619       109       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.74it/s]                                                                            
                 all       5000      36335      0.634      0.479      0.523      0.331

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    89/299     14.2G   0.05091   0.07014   0.02621       150       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.74it/s]                                                                            
                 all       5000      36335      0.637      0.479      0.523      0.331

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    90/299     14.2G   0.05095   0.07052   0.02625        53       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.75it/s]                                                                            
                 all       5000      36335      0.636      0.479      0.523      0.331

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    91/299     14.2G   0.05101   0.07075   0.02629       808       640:  24%|██▍       | 225/925 [01:05<03:20,  3.49it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    91/299     14.2G   0.05096   0.07094   0.02607       122       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.77it/s]                                                                            
                 all       5000      36335      0.634      0.482      0.523      0.331

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    92/299     14.2G   0.05091   0.07012   0.02617        69       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.75it/s]                                                                            
                 all       5000      36335      0.636      0.481      0.523      0.331

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    93/299     14.2G   0.05087   0.07043   0.02616       121       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.69it/s]                                                                            
                 all       5000      36335      0.633      0.482      0.523      0.331

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    94/299     14.2G   0.05084   0.07059   0.02623       860       640:  73%|███████▎  | 673/925 [03:15<01:12,  3.48it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    94/299     14.2G   0.05085   0.07057   0.02619       131       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.76it/s]                                                                            
                 all       5000      36335      0.638      0.479      0.523      0.331

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    95/299     14.2G    0.0509   0.07051   0.02611       146       640: 100%|██████████| 925/925 [04:29<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.76it/s]                                                                            
                 all       5000      36335      0.637       0.48      0.524      0.331

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    96/299     14.2G   0.05085   0.07068   0.02613       104       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.75it/s]                                                                            
                 all       5000      36335      0.636       0.48      0.524      0.331

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    97/299     14.2G   0.05082   0.07071   0.02592       740       640:  40%|████      | 373/925 [01:48<02:38,  3.47it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
    97/299     14.2G   0.05083   0.07055   0.02602        96       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.76it/s]                                                                            
                 all       5000      36335      0.636      0.479      0.524      0.331

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    98/299     14.2G   0.05085   0.07028   0.02594       150       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.76it/s]                                                                            
                 all       5000      36335      0.637       0.48      0.524      0.332

     Epoch   gpu_mem       box       obj       cls    labels  img_size
    99/299     14.2G   0.05071   0.07057   0.02591       110       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.77it/s]                                                                            
                 all       5000      36335      0.635       0.48      0.524      0.332

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   100/299     14.2G   0.05066   0.07101   0.02579       886       640:  39%|███▊      | 357/925 [01:43<02:48,  3.38it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   100/299     14.2G   0.05076   0.07043   0.02589       122       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.74it/s]                                                                            
                 all       5000      36335      0.636       0.48      0.524      0.333

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   101/299     14.2G   0.05067    0.0702   0.02589       100       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.76it/s]                                                                            
                 all       5000      36335      0.637      0.481      0.525      0.333

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   102/299     14.2G   0.05075    0.0701    0.0259       116       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.76it/s]                                                                            
                 all       5000      36335      0.637       0.48      0.525      0.333

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   103/299     14.2G   0.05066   0.06951   0.02569       860       640:   8%|▊         | 75/925 [00:21<04:12,  3.37it/s]                                                                                           train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   103/299     14.2G   0.05072   0.07038   0.02588       141       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.74it/s]                                                                            
                 all       5000      36335      0.635      0.482      0.525      0.333

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   104/299     14.2G   0.05067   0.07051   0.02581       111       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.77it/s]                                                                            
                 all       5000      36335      0.631      0.484      0.525      0.334

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   105/299     14.2G   0.05059   0.07047   0.02558       714       640:  30%|██▉       | 275/925 [01:19<03:08,  3.45it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   105/299     14.2G   0.05055   0.07041   0.02578       115       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.76it/s]                                                                            
                 all       5000      36335      0.633      0.483      0.525      0.334

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   106/299     14.2G   0.05059   0.07019   0.02579       112       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.77it/s]                                                                            
                 all       5000      36335      0.636      0.483      0.525      0.334

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   107/299     14.2G   0.05067   0.07028   0.02561       824       640:  48%|████▊     | 445/925 [02:09<02:17,  3.50it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   107/299     14.2G   0.05059   0.07052   0.02568       103       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.75it/s]                                                                            
                 all       5000      36335      0.637      0.483      0.526      0.334

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   108/299     14.2G   0.05054   0.07029    0.0257       121       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.77it/s]                                                                            
                 all       5000      36335      0.636      0.482      0.526      0.334

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   109/299     14.2G   0.05054   0.07029    0.0257       778       640:  94%|█████████▎| 866/925 [04:11<00:17,  3.37it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   109/299     14.2G   0.05054   0.07032   0.02572        63       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.78it/s]                                                                            
                 all       5000      36335      0.636      0.483      0.527      0.334

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   110/299     14.2G   0.05049   0.07013   0.02566       151       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.75it/s]                                                                            
                 all       5000      36335      0.635      0.484      0.527      0.335

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   111/299     14.2G   0.05052   0.07036   0.02563       118       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.78it/s]                                                                            
                 all       5000      36335      0.634      0.484      0.528      0.336

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   112/299     14.2G   0.05038   0.07024   0.02542       832       640:  35%|███▌      | 328/925 [01:34<02:58,  3.34it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   112/299     14.2G   0.05041   0.07019   0.02555       126       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.77it/s]                                                                            
                 all       5000      36335      0.637      0.483      0.528      0.336

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   113/299     14.2G   0.05048   0.07029   0.02566       119       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.77it/s]                                                                            
                 all       5000      36335      0.638      0.484      0.528      0.336

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   114/299     14.2G   0.05039   0.07006   0.02556       914       640:  83%|████████▎ | 769/925 [03:42<00:44,  3.47it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   114/299     14.2G   0.05043    0.0703   0.02552       106       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.73it/s]                                                                            
                 all       5000      36335      0.642      0.482      0.528      0.336

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   115/299     14.2G   0.05043    0.0704   0.02554        79       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.78it/s]                                                                            
                 all       5000      36335      0.644      0.482      0.528      0.336

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   116/299     14.2G   0.05049   0.07021   0.02554        92       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.77it/s]                                                                            
                 all       5000      36335      0.646      0.482      0.528      0.336

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   117/299     14.2G   0.05039   0.06991   0.02548       721       640:  26%|██▌       | 241/925 [01:09<03:16,  3.48it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   117/299     14.2G    0.0504   0.07003   0.02545       113       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.76it/s]                                                                            
                 all       5000      36335      0.647      0.482      0.528      0.336

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   118/299     14.2G   0.05044   0.07042   0.02545       147       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.76it/s]                                                                            
                 all       5000      36335      0.644      0.483      0.529      0.337

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   119/299     14.2G   0.05043   0.07007   0.02536       873       640:  50%|█████     | 467/925 [02:15<02:14,  3.42it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   119/299     14.2G   0.05043   0.07024   0.02539        92       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.76it/s]                                                                            
                 all       5000      36335      0.646      0.482       0.53      0.338

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   120/299     14.2G   0.05027   0.07004   0.02539        75       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.78it/s]                                                                            
                 all       5000      36335      0.648      0.482       0.53      0.338

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   121/299     14.2G   0.05026   0.07007   0.02528       109       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.77it/s]                                                                            
                 all       5000      36335      0.645      0.483       0.53      0.338

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   122/299     14.2G   0.05043   0.06754   0.02484       766       640:   0%|          | 3/925 [00:00<04:23,  3.50it/s]                                                                                            train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   122/299     14.2G    0.0503   0.07046   0.02539       132       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.643      0.486       0.53      0.338

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   123/299     14.2G   0.05033    0.0701   0.02536        65       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.78it/s]                                                                            
                 all       5000      36335      0.647      0.484       0.53      0.338

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   124/299     14.2G   0.05026    0.0695   0.02522       745       640:  35%|███▍      | 323/925 [01:33<02:55,  3.44it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   124/299     14.2G   0.05022   0.06975   0.02523        87       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.78it/s]                                                                            
                 all       5000      36335      0.644      0.486       0.53      0.338

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   125/299     14.2G   0.05026   0.06971   0.02524        81       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.77it/s]                                                                            
                 all       5000      36335      0.645      0.484       0.53      0.338

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   126/299     14.2G   0.05021   0.06972   0.02523       821       640:  61%|██████▏   | 568/925 [02:44<01:43,  3.45it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   126/299     14.2G   0.05018   0.06988   0.02518       159       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.78it/s]                                                                            
                 all       5000      36335      0.642      0.486      0.531      0.338

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   127/299     14.2G   0.05025   0.06993   0.02519       127       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.77it/s]                                                                            
                 all       5000      36335      0.642      0.486      0.531      0.338

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   128/299     14.2G   0.05025   0.06994   0.02529       801       640:  88%|████████▊ | 811/925 [03:55<00:33,  3.40it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   128/299     14.2G   0.05026   0.06996   0.02531        77       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.73it/s]                                                                            
                 all       5000      36335      0.647      0.484      0.532      0.339

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   129/299     14.2G   0.05011   0.07023    0.0251       120       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.638      0.489      0.532      0.339

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   130/299     14.2G    0.0502      0.07   0.02505       110       640: 100%|██████████| 925/925 [04:29<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.636       0.49      0.532      0.339

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   131/299     14.2G   0.05006   0.07045   0.02538       914       640:   5%|▌         | 50/925 [00:14<04:09,  3.51it/s]                                                                                           train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   131/299     14.2G   0.05011   0.06976   0.02508        99       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.78it/s]                                                                            
                 all       5000      36335      0.638       0.49      0.533      0.339

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   132/299     14.2G   0.05014   0.07035   0.02498        99       640: 100%|██████████| 925/925 [04:29<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.77it/s]                                                                            
                 all       5000      36335      0.636      0.492      0.533       0.34

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   133/299     14.2G   0.04996   0.06963   0.02494       770       640:  28%|██▊       | 255/925 [01:14<03:16,  3.41it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   133/299     14.2G   0.04996    0.0696   0.02507        87       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.78it/s]                                                                            
                 all       5000      36335      0.631      0.494      0.533       0.34

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   134/299     14.2G   0.05009   0.07009   0.02496       109       640: 100%|██████████| 925/925 [04:29<00:00,  3.43it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.78it/s]                                                                            
                 all       5000      36335      0.631      0.495      0.534       0.34

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   135/299     14.2G   0.05008   0.06956   0.02499       740       640:  93%|█████████▎| 859/925 [04:09<00:19,  3.39it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   135/299     14.2G   0.05008   0.06957   0.02499        79       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.77it/s]                                                                            
                 all       5000      36335      0.632      0.495      0.534       0.34

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   136/299     14.2G   0.05003    0.0699    0.0249       100       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.78it/s]                                                                            
                 all       5000      36335      0.632      0.495      0.535      0.341

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   137/299     14.2G   0.04994   0.06982   0.02485        79       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.78it/s]                                                                            
                 all       5000      36335      0.631      0.496      0.535      0.341

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   138/299     14.2G   0.04991   0.06906   0.02507       674       640:  16%|█▌        | 150/925 [00:43<03:42,  3.48it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   138/299     14.2G   0.04993   0.06979   0.02498       132       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.632      0.495      0.535      0.341

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   139/299     14.2G   0.04984    0.0697   0.02488        93       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.76it/s]                                                                            
                 all       5000      36335      0.634      0.494      0.535      0.341

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   140/299     14.2G   0.04981   0.06961   0.02482       790       640:  70%|██████▉   | 644/925 [03:06<01:21,  3.46it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   140/299     14.2G    0.0498   0.06958   0.02483       134       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.632      0.498      0.535      0.341

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   141/299     14.2G   0.04986    0.0696   0.02478       100       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.632      0.498      0.535      0.341

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   142/299     14.2G   0.04978    0.0697   0.02489       889       640:  90%|████████▉ | 832/925 [04:01<00:27,  3.40it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   142/299     14.2G   0.04975    0.0696   0.02486        92       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.636      0.497      0.535      0.342

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   143/299     14.2G   0.04989   0.06988   0.02477       119       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.78it/s]                                                                            
                 all       5000      36335      0.639      0.496      0.535      0.342

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   144/299     14.2G   0.04977   0.06941   0.02473       104       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.638      0.497      0.536      0.342

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   145/299     14.2G   0.04968   0.06993   0.02464       950       640:  21%|██        | 191/925 [00:55<03:31,  3.47it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   145/299     14.2G   0.04985   0.07003   0.02472       122       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.81it/s]                                                                            
                 all       5000      36335       0.64      0.497      0.536      0.343

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   146/299     14.2G   0.04979   0.06964   0.02463       122       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.78it/s]                                                                            
                 all       5000      36335      0.642      0.497      0.537      0.343

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   147/299     14.2G   0.04986   0.07011   0.02456       763       640:  51%|█████     | 468/925 [02:15<02:11,  3.48it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   147/299     14.2G   0.04981   0.06978   0.02461       106       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.641      0.498      0.537      0.343

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   148/299     14.2G    0.0497   0.06975   0.02455        79       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.78it/s]                                                                            
                 all       5000      36335       0.64      0.499      0.538      0.344

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   149/299     14.2G   0.04974   0.06956   0.02454       833       640:  72%|███████▏  | 670/925 [03:14<01:12,  3.51it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   149/299     14.2G    0.0497   0.06954    0.0246        66       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.78it/s]                                                                            
                 all       5000      36335      0.636      0.501      0.538      0.344

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   150/299     14.2G   0.04969   0.06955   0.02437       127       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.642      0.498      0.538      0.344

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   151/299     14.2G   0.04968   0.06956   0.02444       723       640: 100%|█████████▉| 921/925 [04:26<00:01,  3.46it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   151/299     14.2G   0.04968   0.06955   0.02444       104       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.639      0.498      0.538      0.344

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   152/299     14.2G   0.04963   0.06934   0.02447        81       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335       0.63      0.503      0.538      0.344

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   153/299     14.2G   0.04959   0.06933   0.02444       160       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.81it/s]                                                                            
                 all       5000      36335       0.63      0.504      0.538      0.344

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   154/299     14.2G   0.04946   0.06941   0.02417       849       640:  16%|█▋        | 151/925 [00:43<03:44,  3.45it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   154/299     14.2G   0.04952   0.06944   0.02433       109       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.632      0.502      0.539      0.344

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   155/299     14.2G   0.04961   0.06965   0.02444       116       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.634      0.501      0.539      0.345

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   156/299     14.2G   0.04957   0.07002   0.02432      1003       640:  33%|███▎      | 306/925 [01:28<02:56,  3.51it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   156/299     14.2G    0.0495   0.06964   0.02441       133       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.81it/s]                                                                            
                 all       5000      36335      0.633      0.501      0.539      0.345

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   157/299     14.2G   0.04962   0.06959   0.02445       140       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.632      0.502      0.539      0.345

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   158/299     14.2G   0.04948   0.06924   0.02448       726       640:  54%|█████▍    | 504/925 [02:26<02:01,  3.47it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   158/299     14.2G   0.04949   0.06949   0.02447        96       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.77it/s]                                                                            
                 all       5000      36335       0.63      0.504      0.539      0.345

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   159/299     14.2G   0.04946   0.06945   0.02429        89       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.78it/s]                                                                            
                 all       5000      36335      0.631      0.503       0.54      0.345

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   160/299     14.2G   0.04946   0.06922   0.02415       866       640:  73%|███████▎  | 672/925 [03:14<01:15,  3.33it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   160/299     14.2G    0.0495   0.06921    0.0242        69       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.631      0.504       0.54      0.346

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   161/299     14.2G   0.04936   0.06939   0.02422       119       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.627      0.505      0.539      0.345

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   162/299     14.2G   0.04942   0.06967   0.02418       123       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.78it/s]                                                                            
                 all       5000      36335      0.625      0.505      0.539      0.346

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   163/299     14.2G   0.04949   0.06977   0.02394      1004       640:   5%|▍         | 45/925 [00:13<04:16,  3.43it/s]                                                                                           train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   163/299     14.2G   0.04937   0.06917   0.02411       130       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.81it/s]                                                                            
                 all       5000      36335      0.626      0.504      0.539      0.346

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   164/299     14.2G   0.04932   0.06934   0.02403        81       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.78it/s]                                                                            
                 all       5000      36335      0.623      0.507      0.539      0.346

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   165/299     14.2G   0.04925     0.069   0.02399       849       640:  29%|██▊       | 265/925 [01:16<03:06,  3.54it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   165/299     14.2G   0.04931   0.06926     0.024       108       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.625      0.505      0.539      0.346

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   166/299     14.2G    0.0493   0.06925   0.02398       106       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.627      0.504       0.54      0.347

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   167/299     14.2G   0.04922   0.06939   0.02398       828       640:  51%|█████     | 473/925 [02:17<02:13,  3.39it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   167/299     14.2G   0.04923   0.06918   0.02396        80       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.629      0.504       0.54      0.346

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   168/299     14.2G   0.04918   0.06914   0.02393       100       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.631      0.505      0.541      0.347

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   169/299     14.2G   0.04927   0.06943   0.02386       739       640:  70%|███████   | 650/925 [03:08<01:18,  3.51it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   169/299     14.2G   0.04926   0.06946   0.02389       128       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.631      0.506      0.541      0.347

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   170/299     14.2G   0.04913   0.06905   0.02373        90       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.77it/s]                                                                            
                 all       5000      36335      0.629      0.507      0.541      0.347

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   171/299     14.2G   0.04905   0.06866   0.02397       695       640:  89%|████████▊ | 820/925 [03:57<00:29,  3.51it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   171/299     14.2G   0.04905    0.0687   0.02399        62       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335       0.63      0.505      0.541      0.347

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   172/299     14.2G   0.04913   0.06925    0.0238       171       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.628      0.506      0.541      0.347

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   173/299     14.2G   0.04911    0.0692    0.0239        83       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.78it/s]                                                                            
                 all       5000      36335      0.625      0.507      0.541      0.348

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   174/299     14.2G   0.04901   0.06909   0.02376       772       640:   8%|▊         | 73/925 [00:21<04:09,  3.42it/s]                                                                                           train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   174/299     14.2G   0.04911   0.06896   0.02369       121       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.622      0.507      0.542      0.348

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   175/299     14.2G   0.04907   0.06906   0.02373       114       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.622      0.507      0.542      0.348

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   176/299     14.2G   0.04912   0.06915   0.02359       668       640:  30%|██▉       | 274/925 [01:19<03:08,  3.45it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   176/299     14.2G   0.04904   0.06912   0.02368        75       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.625      0.507      0.543      0.348

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   177/299     14.2G   0.04906   0.06913    0.0237       114       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.78it/s]                                                                            
                 all       5000      36335      0.625      0.507      0.543      0.348

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   178/299     14.2G   0.04882   0.06896   0.02364       870       640:  46%|████▌     | 427/925 [02:04<02:29,  3.34it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   178/299     14.2G     0.049   0.06919   0.02359        72       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.629      0.506      0.543      0.348

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   179/299     14.2G   0.04892   0.06883   0.02352       141       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.632      0.505      0.543      0.348

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   180/299     14.2G   0.04891   0.06932   0.02346       791       640:  63%|██████▎   | 585/925 [02:49<01:41,  3.36it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   180/299     14.2G   0.04895   0.06943   0.02353        87       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.634      0.504      0.543      0.349

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   181/299     14.2G   0.04889   0.06867   0.02348        66       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.634      0.505      0.544      0.349

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   182/299     14.2G   0.04879   0.06875   0.02351       825       640:  80%|███████▉  | 739/925 [03:33<00:54,  3.41it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   182/299     14.2G   0.04881   0.06879   0.02354        97       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.636      0.505      0.544      0.349

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   183/299     14.2G   0.04881   0.06909    0.0234        81       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.635      0.506      0.544       0.35

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   184/299     14.2G   0.04878   0.06863   0.02329       891       640:  96%|█████████▌| 890/925 [04:18<00:10,  3.40it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   184/299     14.2G   0.04877   0.06866    0.0233        48       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.77it/s]                                                                            
                 all       5000      36335      0.637      0.505      0.545       0.35

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   185/299     14.2G   0.04877   0.06852   0.02335        94       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.78it/s]                                                                            
                 all       5000      36335      0.642      0.503      0.545       0.35

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   186/299     14.2G   0.04875   0.06876   0.02332        71       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.82it/s]                                                                            
                 all       5000      36335      0.643      0.503      0.545       0.35

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   187/299     14.2G   0.04876   0.06864   0.02333       762       640:  16%|█▌        | 147/925 [00:42<03:46,  3.43it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   187/299     14.2G   0.04873    0.0687   0.02332       105       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.646      0.501      0.545      0.351

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   188/299     14.2G   0.04872   0.06865   0.02311       128       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.645        0.5      0.546      0.351

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   189/299     14.2G   0.04853   0.06829   0.02329       837       640:  36%|███▌      | 335/925 [01:36<02:47,  3.52it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   189/299     14.2G   0.04861   0.06845    0.0232        81       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.646      0.501      0.546      0.351

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   190/299     14.2G   0.04868   0.06834   0.02325       102       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.82it/s]                                                                            
                 all       5000      36335      0.646      0.501      0.546      0.351

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   191/299     14.2G   0.04863   0.06848   0.02314      1009       640:  53%|█████▎    | 493/925 [02:23<02:05,  3.45it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   191/299     14.2G   0.04859   0.06837   0.02314       113       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.646      0.501      0.546      0.351

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   192/299     14.2G   0.04859   0.06856   0.02314       114       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.647      0.501      0.547      0.351

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   193/299     14.2G   0.04853   0.06816   0.02304       688       640:  70%|██████▉   | 646/925 [03:06<01:21,  3.42it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   193/299     14.2G   0.04856   0.06834   0.02303        68       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.78it/s]                                                                            
                 all       5000      36335       0.65      0.501      0.548      0.352

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   194/299     14.2G   0.04852   0.06845     0.023       127       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.651      0.502      0.548      0.352

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   195/299     14.2G   0.04845   0.06841   0.02289       829       640:  89%|████████▊ | 819/925 [03:56<00:30,  3.44it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   195/299     14.2G   0.04849   0.06847   0.02293        68       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.651      0.503      0.548      0.352

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   196/299     14.2G   0.04844   0.06846   0.02289       103       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.81it/s]                                                                            
                 all       5000      36335      0.653      0.501      0.548      0.353

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   197/299     14.2G    0.0484   0.06847   0.02301        91       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.656      0.499      0.548      0.353

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   198/299     14.2G    0.0486   0.06875   0.02264       806       640:   5%|▌         | 50/925 [00:14<04:13,  3.46it/s]                                                                                           train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   198/299     14.2G   0.04841   0.06827    0.0229        93       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.77it/s]                                                                            
                 all       5000      36335      0.651      0.502      0.549      0.354

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   199/299     14.2G   0.04837   0.06864   0.02279       133       640: 100%|██████████| 925/925 [04:29<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.652      0.501      0.549      0.354

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   200/299     14.2G   0.04834   0.06816   0.02287       855       640:  23%|██▎       | 210/925 [01:00<03:23,  3.52it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   200/299     14.2G   0.04831   0.06836   0.02281       113       640: 100%|██████████| 925/925 [04:29<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.81it/s]                                                                            
                 all       5000      36335      0.654      0.501      0.549      0.354

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   201/299     14.2G   0.04827   0.06832   0.02279        90       640: 100%|██████████| 925/925 [04:29<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.81it/s]                                                                            
                 all       5000      36335      0.657      0.501      0.549      0.355

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   202/299     14.2G   0.04827   0.06838   0.02273       766       640:  40%|████      | 371/925 [01:48<02:40,  3.46it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   202/299     14.2G   0.04822   0.06824   0.02267       108       640: 100%|██████████| 925/925 [04:29<00:00,  3.43it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.659        0.5      0.549      0.355

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   203/299     14.2G   0.04813    0.0683   0.02256        80       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.76it/s]                                                                            
                 all       5000      36335      0.656      0.502      0.549      0.355

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   204/299     14.2G   0.04811   0.06777   0.02253       823       640:  56%|█████▋    | 522/925 [02:32<01:59,  3.38it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   204/299     14.2G   0.04813   0.06795   0.02257        89       640: 100%|██████████| 925/925 [04:30<00:00,  3.42it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.82it/s]                                                                            
                 all       5000      36335       0.65      0.505       0.55      0.355

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   205/299     14.2G   0.04806   0.06814   0.02255        97       640: 100%|██████████| 925/925 [04:29<00:00,  3.43it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.652      0.504       0.55      0.355

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   206/299     14.2G    0.0482    0.0685   0.02249       951       640:  74%|███████▍  | 688/925 [03:20<01:10,  3.34it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   206/299     14.2G    0.0482   0.06842   0.02254       248       640: 100%|██████████| 925/925 [04:29<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.651      0.504       0.55      0.355

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   207/299     14.2G   0.04813   0.06836   0.02245        85       640: 100%|██████████| 925/925 [04:29<00:00,  3.43it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.81it/s]                                                                            
                 all       5000      36335      0.658      0.499       0.55      0.355

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   208/299     14.2G   0.04812   0.06823   0.02248      1000       640:  92%|█████████▏| 852/925 [04:07<00:21,  3.44it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   208/299     14.2G   0.04812   0.06823   0.02249       145       640: 100%|██████████| 925/925 [04:29<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.81it/s]                                                                            
                 all       5000      36335      0.654      0.502      0.551      0.356

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   209/299     14.2G     0.048   0.06784   0.02235       970       640:  93%|█████████▎| 864/925 [04:10<00:18,  3.38it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   209/299     14.2G     0.048   0.06779   0.02236        79       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.651      0.504      0.551      0.356

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   210/299     14.2G   0.04792   0.06799   0.02237       150       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.82it/s]                                                                            
                 all       5000      36335      0.649      0.505      0.551      0.356

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   211/299     14.2G   0.04792   0.06792   0.02232       131       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.645      0.508      0.552      0.356

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   212/299     14.2G   0.04791   0.06801   0.02225       102       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.644      0.508      0.552      0.357

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   213/299     14.2G   0.04795   0.06786   0.02222        82       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.81it/s]                                                                            
                 all       5000      36335      0.646      0.509      0.553      0.357

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   214/299     14.2G   0.04776   0.06754   0.02227       923       640:  26%|██▌       | 241/925 [01:10<03:17,  3.47it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   214/299     14.2G   0.04777   0.06745   0.02224       185       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.647      0.511      0.553      0.357

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   215/299     14.2G    0.0478   0.06781   0.02212        93       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.648      0.509      0.552      0.357

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   216/299     14.2G    0.0478   0.06778   0.02219       867       640:  44%|████▎     | 404/925 [01:57<02:32,  3.42it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   216/299     14.2G   0.04778   0.06791   0.02216       142       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.652      0.509      0.553      0.357

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   217/299     14.2G   0.04779   0.06769   0.02204       162       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.81it/s]                                                                            
                 all       5000      36335       0.65       0.51      0.553      0.357

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   218/299     14.2G   0.04763   0.06772   0.02209       808       640:  61%|██████▏   | 568/925 [02:45<01:42,  3.48it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   218/299     14.2G   0.04766   0.06768   0.02208       164       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.81it/s]                                                                            
                 all       5000      36335      0.649      0.511      0.553      0.358

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   219/299     14.2G   0.04776   0.06799   0.02199        81       640: 100%|██████████| 925/925 [04:29<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.78it/s]                                                                            
                 all       5000      36335       0.65       0.51      0.554      0.358

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   220/299     14.2G   0.04762   0.06771    0.0221       838       640:  79%|███████▉  | 733/925 [03:33<00:57,  3.33it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   220/299     14.2G   0.04767   0.06773   0.02208       103       640: 100%|██████████| 925/925 [04:29<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:21<00:00,  1.82it/s]                                                                            
                 all       5000      36335       0.65       0.51      0.554      0.358

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   221/299     14.2G   0.04761   0.06762   0.02186       888       640:  42%|████▏     | 386/925 [01:52<02:37,  3.43it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   221/299     14.2G   0.04763   0.06771   0.02186       110       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.81it/s]                                                                            
                 all       5000      36335      0.649      0.511      0.554      0.359

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   222/299     14.2G   0.04766   0.06764   0.02189        75       640: 100%|██████████| 925/925 [04:29<00:00,  3.43it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:21<00:00,  1.82it/s]                                                                            
                 all       5000      36335      0.653       0.51      0.554      0.359

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   223/299     14.2G   0.04759   0.06756   0.02168        71       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.651      0.512      0.554      0.359

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   224/299     14.2G    0.0475   0.06752   0.02181       946       640:  60%|██████    | 558/925 [02:42<01:47,  3.42it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   224/299     14.2G   0.04751   0.06742   0.02181       102       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.76it/s]                                                                            
                 all       5000      36335      0.652      0.513      0.554      0.359

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   225/299     14.2G    0.0474   0.06779   0.02171        96       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.81it/s]                                                                            
                 all       5000      36335      0.653      0.512      0.554      0.359

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   226/299     14.2G   0.04744   0.06723   0.02169        98       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.647      0.517      0.555       0.36

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   227/299     14.2G   0.04737   0.06737   0.02157       869       640: 100%|█████████▉| 922/925 [04:27<00:00,  3.48it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   227/299     14.2G   0.04737   0.06736   0.02158        99       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.648      0.516      0.555      0.359

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   228/299     14.2G   0.04733   0.06751   0.02155       118       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.81it/s]                                                                            
                 all       5000      36335      0.648      0.516      0.555       0.36

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   229/299     14.2G    0.0473    0.0676   0.02156        77       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.649      0.517      0.555       0.36

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   230/299     14.2G   0.04723   0.06693   0.02136       955       640:  40%|███▉      | 368/925 [01:47<02:41,  3.46it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   230/299     14.2G   0.04721   0.06695   0.02136       129       640: 100%|██████████| 925/925 [04:29<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.81it/s]                                                                            
                 all       5000      36335      0.648      0.517      0.556       0.36

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   231/299     14.2G   0.04722   0.06749   0.02139        79       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:21<00:00,  1.82it/s]                                                                            
                 all       5000      36335      0.649      0.516      0.556       0.36

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   232/299     14.2G   0.04724   0.06727   0.02139       835       640:  70%|███████   | 652/925 [03:09<01:20,  3.39it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   232/299     14.2G   0.04723   0.06745   0.02136       148       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.82it/s]                                                                            
                 all       5000      36335      0.655      0.514      0.556       0.36

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   233/299     14.2G   0.04721   0.06727   0.02131        95       640: 100%|██████████| 925/925 [04:29<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.81it/s]                                                                            
                 all       5000      36335      0.653      0.515      0.556      0.361

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   234/299     14.2G   0.04712   0.06694   0.02133       774       640:  89%|████████▊ | 819/925 [03:58<00:31,  3.35it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   234/299     14.2G   0.04711     0.067   0.02136        72       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.654      0.515      0.557      0.361

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   235/299     14.2G   0.04717    0.0677   0.02127        74       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.81it/s]                                                                            
                 all       5000      36335      0.653      0.515      0.557      0.362

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   236/299     14.2G   0.04705   0.06722   0.02119       111       640: 100%|██████████| 925/925 [04:29<00:00,  3.43it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.649      0.517      0.558      0.362

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   237/299     14.2G   0.04706   0.06741   0.02109        90       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.81it/s]                                                                            
                 all       5000      36335      0.649      0.517      0.558      0.362

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   238/299     14.2G   0.04694   0.06676   0.02109       749       640:   5%|▍         | 43/925 [00:12<04:14,  3.47it/s]                                                                                           train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   238/299     14.2G   0.04693   0.06714   0.02101       113       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:21<00:00,  1.82it/s]                                                                            
                 all       5000      36335      0.654      0.515      0.557      0.362

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   239/299     14.2G   0.04691    0.0669   0.02099       107       640: 100%|██████████| 925/925 [04:29<00:00,  3.43it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:21<00:00,  1.83it/s]                                                                            
                 all       5000      36335      0.658      0.514      0.558      0.362

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   240/299     14.2G   0.04683   0.06705   0.02099       718       640:  83%|████████▎ | 764/925 [03:41<00:46,  3.45it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   240/299     14.2G   0.04682     0.067   0.02097        69       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:21<00:00,  1.82it/s]                                                                            
                 all       5000      36335      0.661      0.513      0.558      0.362

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   241/299     14.2G   0.04675   0.06692   0.02088       125       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.661      0.512      0.558      0.363

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   242/299     14.2G   0.04674   0.06701   0.02081       115       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:21<00:00,  1.82it/s]                                                                            
                 all       5000      36335      0.659      0.515      0.559      0.363

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   243/299     14.2G   0.04669   0.06653   0.02076       854       640:  49%|████▊     | 450/925 [02:10<02:16,  3.47it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   243/299     14.2G   0.04666   0.06672   0.02077        77       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:21<00:00,  1.82it/s]                                                                            
                 all       5000      36335      0.661      0.515      0.559      0.363

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   244/299     14.2G   0.04673    0.0671   0.02075       170       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.655      0.519      0.559      0.363

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   245/299     14.2G    0.0466   0.06669   0.02068       754       640:  74%|███████▎  | 681/925 [03:17<01:10,  3.45it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   245/299     14.2G   0.04659   0.06677   0.02068        72       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.81it/s]                                                                            
                 all       5000      36335      0.658      0.517      0.559      0.363

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   246/299     14.2G   0.04668   0.06687   0.02065       126       640: 100%|██████████| 925/925 [04:29<00:00,  3.43it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:21<00:00,  1.82it/s]                                                                            
                 all       5000      36335      0.658      0.519       0.56      0.364

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   247/299     14.2G   0.04645   0.06654   0.02063        95       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.81it/s]                                                                            
                 all       5000      36335      0.658      0.518       0.56      0.364

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   248/299     14.2G   0.04649   0.06673   0.02066       824       640:  41%|████▏     | 382/925 [01:50<02:37,  3.45it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   248/299     14.2G   0.04645   0.06658   0.02052        84       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:21<00:00,  1.83it/s]                                                                            
                 all       5000      36335      0.657       0.52      0.561      0.364

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   249/299     14.2G   0.04645   0.06636   0.02042        81       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:21<00:00,  1.84it/s]                                                                            
                 all       5000      36335      0.661      0.519      0.561      0.365

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   250/299     14.2G   0.04657   0.06683   0.02042       775       640:  60%|██████    | 559/925 [02:41<01:45,  3.46it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   250/299     14.2G   0.04652   0.06673   0.02042        76       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:21<00:00,  1.83it/s]                                                                            
                 all       5000      36335      0.663      0.519      0.562      0.365

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   251/299     14.2G   0.04631    0.0664   0.02026        72       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.81it/s]                                                                            
                 all       5000      36335      0.663      0.519      0.562      0.365

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   252/299     14.2G   0.04633   0.06634   0.02029       848       640:  85%|████████▍ | 784/925 [03:47<00:40,  3.45it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   252/299     14.2G    0.0463   0.06638   0.02028        73       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:21<00:00,  1.83it/s]                                                                            
                 all       5000      36335      0.666      0.518      0.562      0.365

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   253/299     14.2G   0.04618   0.06623   0.02009       141       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.81it/s]                                                                            
                 all       5000      36335      0.664      0.518      0.562      0.365

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   254/299     14.2G   0.04614   0.06621   0.02021        76       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.82it/s]                                                                            
                 all       5000      36335      0.667      0.519      0.562      0.365

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   255/299     14.2G   0.04579    0.0652   0.02019       784       640:   8%|▊         | 76/925 [00:22<04:03,  3.48it/s]                                                                                           train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   255/299     14.2G   0.04614     0.066    0.0201        75       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:21<00:00,  1.84it/s]                                                                            
                 all       5000      36335      0.664      0.519      0.563      0.365

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   256/299     14.2G   0.04609   0.06641   0.02011       147       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:21<00:00,  1.84it/s]                                                                            
                 all       5000      36335      0.665      0.519      0.563      0.366

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   257/299     14.2G   0.04596   0.06613   0.01978       881       640:  34%|███▍      | 313/925 [01:30<02:56,  3.47it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   257/299     14.2G   0.04604   0.06631   0.01987        96       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:21<00:00,  1.85it/s]                                                                            
                 all       5000      36335      0.667      0.518      0.562      0.365

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   258/299     14.2G   0.04596   0.06608   0.01983       144       640: 100%|██████████| 925/925 [04:29<00:00,  3.43it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:21<00:00,  1.82it/s]                                                                            
                 all       5000      36335      0.666      0.518      0.563      0.366

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   259/299     14.2G   0.04585   0.06595   0.01973       921       640:  51%|█████     | 470/925 [02:16<02:11,  3.47it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   259/299     14.2G   0.04587   0.06592    0.0198       121       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:21<00:00,  1.83it/s]                                                                            
                 all       5000      36335      0.668       0.52      0.563      0.366

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   260/299     14.2G   0.04591   0.06629    0.0197        93       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:21<00:00,  1.84it/s]                                                                            
                 all       5000      36335      0.668      0.519      0.563      0.367

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   261/299     14.2G   0.04584   0.06607    0.0197       836       640:  68%|██████▊   | 630/925 [03:03<01:24,  3.49it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   261/299     14.2G   0.04581   0.06597   0.01969        52       640: 100%|██████████| 925/925 [04:29<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:21<00:00,  1.82it/s]                                                                            
                 all       5000      36335      0.664      0.522      0.563      0.367

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   262/299     14.2G    0.0457   0.06561   0.01954        89       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:21<00:00,  1.84it/s]                                                                            
                 all       5000      36335      0.664      0.522      0.563      0.367

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   263/299     14.2G   0.04565   0.06563   0.01953       756       640:  86%|████████▌ | 794/925 [03:50<00:38,  3.39it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   263/299     14.2G   0.04565   0.06568   0.01953       113       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:21<00:00,  1.83it/s]                                                                            
                 all       5000      36335      0.666       0.52      0.564      0.368

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   264/299     14.2G   0.04564   0.06567    0.0194        93       640: 100%|██████████| 925/925 [04:29<00:00,  3.43it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:21<00:00,  1.85it/s]                                                                            
                 all       5000      36335      0.668       0.52      0.564      0.368

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   265/299     14.2G   0.04556   0.06547   0.01945        68       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.81it/s]                                                                            
                 all       5000      36335      0.665       0.52      0.564      0.368

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   266/299     14.2G   0.04546   0.06643   0.01949       840       640:   7%|▋         | 66/925 [00:19<04:09,  3.44it/s]                                                                                           train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   266/299     14.2G   0.04543   0.06555   0.01937       117       640: 100%|██████████| 925/925 [04:29<00:00,  3.43it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:21<00:00,  1.84it/s]                                                                            
                 all       5000      36335      0.663       0.52      0.565      0.368

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   267/299     14.2G   0.04553   0.06553   0.01932        56       640: 100%|██████████| 925/925 [04:29<00:00,  3.43it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:21<00:00,  1.84it/s]                                                                            
                 all       5000      36335      0.663      0.521      0.565      0.369

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   268/299     14.2G   0.04551   0.06526   0.01942       960       640:  24%|██▎       | 219/925 [01:03<03:25,  3.44it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   268/299     14.2G   0.04534   0.06525   0.01921        83       640: 100%|██████████| 925/925 [04:29<00:00,  3.43it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:21<00:00,  1.84it/s]                                                                            
                 all       5000      36335      0.666      0.519      0.565      0.368

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   269/299     14.2G   0.04534   0.06556   0.01913       107       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:21<00:00,  1.83it/s]                                                                            
                 all       5000      36335      0.667      0.519      0.565      0.368

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   270/299     14.2G   0.04525   0.06478   0.01892       867       640:  42%|████▏     | 390/925 [01:53<02:35,  3.44it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   270/299     14.2G   0.04529   0.06514   0.01899        81       640: 100%|██████████| 925/925 [04:29<00:00,  3.43it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.75it/s]                                                                            
                 all       5000      36335      0.667      0.519      0.564      0.369

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   271/299     14.2G   0.04523   0.06539   0.01897       115       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.77it/s]                                                                            
                 all       5000      36335      0.667      0.519      0.565      0.369

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   272/299     14.2G   0.04508   0.06472   0.01891       824       640:  63%|██████▎   | 584/925 [02:49<01:39,  3.44it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   272/299     14.2G    0.0451   0.06473    0.0189        86       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.78it/s]                                                                            
                 all       5000      36335      0.667      0.519      0.565      0.369

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   273/299     14.2G   0.04505   0.06512   0.01887        87       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:21<00:00,  1.83it/s]                                                                            
                 all       5000      36335      0.661      0.523      0.565      0.369

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   274/299     14.2G     0.045   0.06498   0.01857       674       640:  86%|████████▌ | 792/925 [03:49<00:38,  3.42it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   274/299     14.2G   0.04499   0.06486   0.01862        66       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.663      0.521      0.566      0.369

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   275/299     14.2G     0.045   0.06513   0.01863       153       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.78it/s]                                                                            
                 all       5000      36335      0.663      0.522      0.566       0.37

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   276/299     14.2G   0.04489   0.06469   0.01852       120       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.78it/s]                                                                            
                 all       5000      36335      0.657      0.528      0.567       0.37

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   277/299     14.2G    0.0451   0.06499   0.01856       952       640:   6%|▌         | 53/925 [00:15<04:08,  3.50it/s]                                                                                           train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   277/299     14.2G   0.04482   0.06482    0.0185        86       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.658      0.528      0.567       0.37

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   278/299     14.2G   0.04466   0.06456   0.01839       131       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.662      0.527      0.567       0.37

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   279/299     14.2G   0.04466   0.06459   0.01837       787       640:  23%|██▎       | 215/925 [01:02<03:23,  3.48it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   279/299     14.2G   0.04469   0.06478   0.01827       160       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.658      0.529      0.568      0.371

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   280/299     14.2G   0.04453   0.06462   0.01822       102       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.654       0.53      0.568      0.371

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   281/299     14.2G   0.04456   0.06476   0.01802       863       640:  44%|████▍     | 411/925 [01:59<02:30,  3.42it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   281/299     14.2G   0.04452   0.06457   0.01808       120       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.655       0.53      0.568      0.371

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   282/299     14.2G   0.04437   0.06418   0.01804       175       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.658      0.528      0.568      0.371

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   283/299     14.2G   0.04438   0.06444   0.01789       938       640:  77%|███████▋  | 715/925 [03:26<01:00,  3.50it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   283/299     14.2G   0.04432    0.0644    0.0179       100       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.658      0.528      0.568      0.371

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   284/299     14.2G   0.04429   0.06402   0.01795       106       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.81it/s]                                                                            
                 all       5000      36335      0.661      0.526      0.568      0.371

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   285/299     14.2G   0.04419   0.06386   0.01773        91       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.74it/s]                                                                            
                 all       5000      36335      0.663      0.527      0.569      0.372

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   286/299     14.2G   0.04304   0.06241    0.0174       826       640:   1%|          | 6/925 [00:01<04:23,  3.49it/s]                                                                                            train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   286/299     14.2G   0.04409   0.06402   0.01771       124       640: 100%|██████████| 925/925 [04:26<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.73it/s]                                                                            
                 all       5000      36335      0.661      0.529      0.569      0.372

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   287/299     14.2G     0.044   0.06363   0.01762       101       640: 100%|██████████| 925/925 [04:29<00:00,  3.43it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.75it/s]                                                                            
                 all       5000      36335       0.66      0.528      0.569      0.372

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   288/299     14.2G     0.044   0.06378   0.01746        69       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.77it/s]                                                                            
                 all       5000      36335      0.662      0.528      0.569      0.372

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   289/299     14.2G   0.04423   0.06203   0.01834       818       640:   0%|          | 3/925 [00:00<04:21,  3.53it/s]                                                                                            train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   289/299     14.2G   0.04378   0.06323   0.01741        52       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:23<00:00,  1.72it/s]                                                                            
                 all       5000      36335      0.659      0.529       0.57      0.372

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   290/299     14.2G   0.04375   0.06362   0.01729        90       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.81it/s]                                                                            
                 all       5000      36335      0.658      0.529      0.569      0.372

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   291/299     14.2G   0.04367   0.06346   0.01721       930       640:  21%|██▏       | 198/925 [00:57<03:28,  3.49it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   291/299     14.2G   0.04368   0.06342   0.01728       187       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.659      0.529      0.569      0.372

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   292/299     14.2G    0.0437   0.06343   0.01715       135       640: 100%|██████████| 925/925 [04:29<00:00,  3.43it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.661      0.528      0.569      0.372

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   293/299     14.2G   0.04365    0.0635   0.01706       844       640:  84%|████████▍ | 777/925 [03:44<00:43,  3.40it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   293/299     14.2G   0.04361   0.06327   0.01706        72       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.75it/s]                                                                            
                 all       5000      36335      0.661      0.529      0.569      0.372

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   294/299     14.2G   0.04349   0.06313   0.01699       100       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.662      0.529      0.569      0.372

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   295/299     14.2G    0.0434   0.06297   0.01686       123       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.81it/s]                                                                            
                 all       5000      36335      0.664      0.528      0.569      0.372

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   296/299     14.2G   0.04328   0.06323   0.01674       758       640:  24%|██▍       | 221/925 [01:03<03:21,  3.49it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   296/299     14.2G    0.0433   0.06308   0.01675       130       640: 100%|██████████| 925/925 [04:27<00:00,  3.46it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335      0.662      0.529      0.569      0.372

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   297/299     14.2G   0.04323   0.06276   0.01668       100       640: 100%|██████████| 925/925 [04:28<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.662      0.529      0.569      0.372

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   298/299     14.2G   0.04317   0.06322   0.01655       832       640:  43%|████▎     | 394/925 [01:54<02:34,  3.43it/s]                                                                                          train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
train.py:317: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
   298/299     14.2G   0.04314   0.06296   0.01658        99       640: 100%|██████████| 925/925 [04:27<00:00,  3.45it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.79it/s]                                                                            
                 all       5000      36335       0.66      0.531       0.57      0.373

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   299/299     14.2G   0.04303   0.06252   0.01647        89       640: 100%|██████████| 925/925 [04:28<00:00,  3.44it/s]                                                                                          
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:22<00:00,  1.80it/s]                                                                            
                 all       5000      36335      0.662       0.53       0.57      0.372

300 epochs completed in 24.387 hours.
Optimizer stripped from runs/train/exp/weights/last.pt, 14.8MB
Optimizer stripped from runs/train/exp/weights/best.pt, 14.8MB

Validating runs/train/exp/weights/best.pt...
Fusing layers... 
Model summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 40/40 [00:39<00:00,  1.01it/s]                                                                            
                 all       5000      36335      0.661      0.525      0.565      0.373
              person       5000      10777      0.732      0.703      0.757      0.491
             bicycle       5000        314      0.621      0.484      0.532      0.296
                 car       5000       1918      0.635      0.588       0.62      0.382
          motorcycle       5000        367      0.705      0.614      0.672      0.389
            airplane       5000        143      0.827      0.797      0.863      0.614
                 bus       5000        283      0.817      0.703      0.769      0.603
               train       5000        190      0.846      0.726      0.828      0.583
               truck       5000        414      0.607      0.452      0.499      0.317
                boat       5000        424      0.585      0.392      0.433      0.205
       traffic light       5000        634      0.642      0.489      0.513      0.249
        fire hydrant       5000        101      0.872      0.733      0.808      0.623
           stop sign       5000         75      0.838       0.68      0.737      0.627
       parking meter       5000         60      0.696      0.572      0.632      0.441
               bench       5000        411      0.607      0.282      0.325      0.201
                bird       5000        427      0.601      0.412       0.47      0.283
                 cat       5000        202      0.805      0.795      0.834       0.58
                 dog       5000        218      0.745      0.679       0.71      0.535
               horse       5000        272      0.761      0.702      0.764      0.541
               sheep       5000        354      0.642      0.715      0.718      0.477
                 cow       5000        372      0.695      0.669      0.729      0.492
            elephant       5000        252      0.722      0.833      0.801      0.561
                bear       5000         71      0.799      0.817      0.846       0.67
               zebra       5000        266      0.806      0.823       0.88      0.638
             giraffe       5000        232      0.874      0.836        0.9      0.674
            backpack       5000        371      0.477      0.243      0.257       0.13
            umbrella       5000        407      0.649       0.58      0.588      0.352
             handbag       5000        540      0.533      0.207      0.226      0.114
                 tie       5000        252      0.669      0.452      0.479       0.27
            suitcase       5000        299      0.608      0.505      0.547      0.345
             frisbee       5000        115      0.814      0.809       0.85      0.621
                skis       5000        241      0.651      0.382      0.433      0.199
           snowboard       5000         69      0.554      0.406       0.44      0.259
         sports ball       5000        260      0.689      0.592      0.614      0.404
                kite       5000        327      0.628      0.599      0.615      0.392
        baseball bat       5000        145      0.658      0.491      0.488      0.249
      baseball glove       5000        148      0.623      0.554      0.582      0.341
          skateboard       5000        179      0.759      0.665       0.72      0.469
           surfboard       5000        267      0.639      0.502      0.522      0.294
       tennis racket       5000        225      0.699      0.698      0.726      0.423
              bottle       5000       1013      0.592      0.471       0.52      0.322
          wine glass       5000        341      0.684      0.449      0.525      0.309
                 cup       5000        895      0.615      0.537      0.568      0.387
                fork       5000        215      0.646      0.358      0.423      0.255
               knife       5000        325      0.516      0.212      0.245      0.125
               spoon       5000        253      0.442      0.186      0.201      0.111
                bowl       5000        623      0.622      0.491      0.538      0.365
              banana       5000        370      0.497      0.368      0.351        0.2
               apple       5000        236      0.469      0.263      0.253      0.164
            sandwich       5000        177      0.591      0.441      0.482      0.333
              orange       5000        285      0.502      0.404      0.402      0.296
            broccoli       5000        312      0.496      0.401      0.403      0.202
              carrot       5000        365       0.44      0.356      0.332      0.192
             hot dog       5000        125      0.745      0.472      0.507      0.323
               pizza       5000        284      0.717      0.632      0.684      0.471
               donut       5000        328      0.561      0.522      0.526      0.401
                cake       5000        310      0.659      0.458      0.529      0.325
               chair       5000       1771      0.597      0.398      0.446      0.264
               couch       5000        261      0.715        0.5      0.594      0.399
        potted plant       5000        342      0.516      0.398       0.42      0.228
                 bed       5000        163      0.726      0.479      0.591       0.39
        dining table       5000        695      0.571      0.348      0.381      0.237
              toilet       5000        179      0.763      0.721        0.8      0.615
                  tv       5000        288      0.738      0.674      0.752      0.525
              laptop       5000        231      0.763      0.696      0.739      0.556
               mouse       5000        106       0.69      0.736      0.764       0.55
              remote       5000        283      0.521      0.417      0.427      0.231
            keyboard       5000        153      0.631      0.613      0.663      0.442
          cell phone       5000        262      0.573      0.477      0.486      0.315
           microwave       5000         55      0.731      0.727      0.751      0.541
                oven       5000        143      0.607      0.455      0.527      0.334
             toaster       5000          9      0.843        0.6      0.649      0.479
                sink       5000        225      0.596      0.493      0.528      0.346
        refrigerator       5000        126      0.758      0.563      0.677       0.48
                book       5000       1129      0.441      0.205      0.226     0.0983
               clock       5000        267      0.747      0.682      0.728      0.484
                vase       5000        274      0.581      0.511      0.521      0.349
            scissors       5000         36       0.52      0.241      0.323      0.213
          teddy bear       5000        190      0.726      0.547      0.622      0.402
          hair drier       5000         11          1          0    0.00324    0.00104
          toothbrush       5000         57      0.583      0.316      0.379      0.206

Evaluating pycocotools mAP... saving runs/train/exp/_predictions.json...
loading annotations into memory...
Done (t=1.02s)
creating index...
index created!
Loading and preparing results...
DONE (t=5.91s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=77.27s).
Accumulating evaluation results...
DONE (t=15.65s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.376
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.572
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.406
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.220
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.424
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.483
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.310
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.515
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.570
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.380
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.629
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.713
Results saved to runs/train/exp
[INFO] 2022-08-22 17:54:21,874 api: [default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
[INFO] 2022-08-22 17:54:21,876 api: Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
/root/miniconda3/lib/python3.8/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
[INFO] 2022-08-22 17:54:21,879 api: Done waiting for other agents. Elapsed: 0.001999378204345703 seconds
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "48236", "role": "default", "hostname": "container-31ed11a53c-9cbe465a", "state": "SUCCEEDED", "total_run_time": 88000, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "48237", "role": "default", "hostname": "container-31ed11a53c-9cbe465a", "state": "SUCCEEDED", "total_run_time": 88000, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "container-31ed11a53c-9cbe465a", "state": "SUCCEEDED", "total_run_time": 88000, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 0}}
